#!/usr/bin/env python3

# Copyright (C) 2018, Amazon.com. All Rights Reserved
#
# Kaena neural network executor with mapping to mixed backends.
#

import argparse
import os.path
import sys, json, re, getpass, random
import glob
import shutil
import socket, errno
import datetime
kPath = os.environ.get('KAENA_PATH')
if kPath is None:
  kPath =''
sys.path.insert(0, kPath + "/compiler")

sys.path.insert(0, "/usr/lib/python3.6/site-packages/")
import numpy as np
from tffe.NpTransforms import NpTrans as npt
from tffe.NpTransforms import TensorFormat, TensorFormatMap

def getInfoDateStr():
  return "INFO %s :" % str(datetime.datetime.now())


print("\n", getInfoDateStr(), "started as  ", " ".join(sys.argv), flush=True)

parser = argparse.ArgumentParser()
parser.add_argument('--kelf_dir', help='kelf directory', default="")
parser.add_argument('--tfpb', help='TensorFlow freeze graph file', default="f.pb")
parser.add_argument('--input_files', help='Files to be passed in to the first subgraph (filenames or node=filename pairs)', default=[], nargs='+')
parser.add_argument('--output_files', help='Specify output files for last subgraph', default=[], nargs='+')
parser.add_argument('--check_against_ref', help='Compare produced subgraph output npy files against compiler provided goldens in local directory; use none (defalt), last, all, all_available, all_internal', default='none')
parser.add_argument('--working_dir', help='all output will be stored here', default="")
parser.add_argument('--inkling_debugflags', help='inkling extra args', default=[], nargs='*')
parser.add_argument('--env', help='Set envvars pairs var=val var1=val1. Used to pass experimental controls to low-level tools', nargs='+', default=[])
parser.add_argument('--cached_goldens', help='Path to a cached runs. New files created (and diffed against TF) in this run are also compared against the new file in the cached run', default=None)
parser.add_argument('--diff_options', help='String passed to npy_diff_files utility, such as to control tolerance, default is ""', default="")
parser.add_argument('--diff_options_cached', help='As diff options, but used for --cached_goldens. Default is "--binary_match"', default='--binary_match')

args = parser.parse_args()
kelf_dir = args.kelf_dir
kelf_dir = os.path.abspath(kelf_dir)
assert(kelf_dir != "")
nnGraphFile = kelf_dir + "/nn_graph.json"

# The --env support
if len(args.env) > 0:
  for varEqVal in args.env:
    var, val = varEqVal.split("=")
    os.environ[var] = val

def parseInputFilesArg(arg, input_files):
  for i in range(len(args.input_files)):
    if '=' in args.input_files[i]:
      (node, fname) = args.input_files[i].split("=", 1)
    else:
      node = i
      fname = args.input_files[i]
    input_files[node] = os.path.abspath(fname)

input_files = {}
parseInputFilesArg(args.input_files, input_files)

output_files = [os.path.abspath(f) for f in args.output_files]
working_dir = args.working_dir
if (working_dir == ""):
  working_dir = "%s/working_dir" % kelf_dir
working_dir = os.path.abspath(working_dir)

if os.path.exists(working_dir):
  shutil.rmtree(working_dir)
os.makedirs(working_dir)

print("INFO: using working directory  %s" % (working_dir))

class NnSubgraph:
  def __init__(self, sgId, sgJson):
    self.sgId = sgId
    self.sgDir = sgJson["SubGraphDir"]
    self.inputs = sgJson["Inputs"]
    self.outputs = sgJson["Outputs"]
    self.executor = sgJson["executor"]
    self.execOptions = sgJson.get("execOptions", "")
    self.parallel_streams = sgJson.get("parallel_streams", False)
    self.tensorFormatMap = TensorFormatMap()
    self.cmd = ""
    self.cmd_args = ""
    if self.executor == "processor":
      self.cmd = sgJson["cmd"]
      self.cmd_args = sgJson["cmd_args"]
    if re.search(r'wave', self.executor):
      self.tensorFormatMap.readJson("%s/%s/tensor_map.json" % (kelf_dir, self.sgDir))

  # Translate and relink npy files that are inputs to this subgraph
  # Each subgraph produces output in the host format. So just rename npy
  # files A:0.npy to point to ../A:0-out.npy
  def getUpdatedInputs(self):
    newInputs = []
    if self.sgId == 0:
      return self.inputs

    for input in self.inputs:
      name = input["name"]
      f = input["file"]
      tfFile = "%s" % f.replace(".npy", "-out.npy")
      newInputs += [{"name" : name, "file" :tfFile}]

    return newInputs

  # Returns input file names in TPB format
  def createTpbFormatInputs(self):
    inputFilesInTpbFotmat = []
    for input in self.inputs:
      name = input["name"]
      f = input["file"]
      tff = self.tensorFormatMap.get(name)
      assert tff != None
      tpbInpFile = tff.simFile
      if self.sgId == 0:
        # The inputs for the first subgraph are same as when compiled
        tfFile = f
      else:
        assert f == tff.tfFile
        # For next sungraph it is teh output of previous subgraph (hence -out.npy)
        tfFile = f.replace(".npy", "-out.npy")

      if os.path.isfile(tpbInpFile):
        # Move the compiler generated input file
        origFile = tpbInpFile.replace(".npy", "-orig.npy")
        os.rename(tpbInpFile, origFile)

      # Translate and write the previous subgraph file
      if os.path.isfile(tfFile):
        inputShapeTf = input["shape"]
        inputFormatTf = tff.tfFormat
        inputFormatSim = tff.simFormat
        if self.sgId == 0:
          # The inputs for the first subgraph are same as when compiled

          ## # kaena-602: hack for testing one input file only; doesn't work for inferencing
          tpbInpPadSplitFile = tpbInpFile[:-4] + "_padsplit.npy"
          if os.path.isfile(tpbInpPadSplitFile):
            print("nn_executor: Replacing input npy ", tpbInpFile, " with padded input ", tpbInpPadSplitFile)
            tpbInpFile = tpbInpPadSplitFile
          else:
            npt.formatNpyFileAs(tfFile, inputFormatTf, inputFormatSim, outFile=tpbInpFile)
            print("nn_executor: Using original input npy ", tpbInpFile, ", did not find padded input ", tpbInpPadSplitFile)
            print("INFO: nn_executor: did not find padded input therfore translated TF format %s %s  to TPB Fmap format %s %s" % (inputFormatTf, tfFile, inputFormatSim, tpbInpFile))

          assert os.path.isfile(tpbInpFile)  # exists from compilation
        else:
          npt.formatNpyFileAs(tfFile, inputFormatTf, inputFormatSim, outFile=tpbInpFile)
          print(getInfoDateStr(), "nn_executor: translated TF format %s %s  to TPB Fmap format %s %s" % (inputFormatTf, tfFile, inputFormatSim, tpbInpFile))
        inputFilesInTpbFotmat.append(tpbInpFile)
      else:
        print("ERROR: no tf file not found %s" % tfFile, flush=True)
        sys.exit(-1)
    return inputFilesInTpbFotmat

  # Constant npy files like weights are crated by compiler (tffe)
  # in sg* and thus linked for running inference (here, which is <test>/working_dir)
  def linkConsts(self):
    for f in self.tensorFormatMap.getConstFilesSim():
      src_f = "%s/%s/%s" % (kelf_dir, self.sgDir, f)
      print("DEBUG3 linking", src_f, f)
      os.symlink(src_f, f)
    # ME creates files of constants that are not visible to TFFE, so link them too
    files =  glob.glob("../%s/*-constants.npy" % self.sgDir)
    files += glob.glob("../%s/*_padsplit.npy" % self.sgDir)
    files += glob.glob("../%s/*_concat_weight*.npy" % self.sgDir)
    for f in files:
      dst_f = os.path.basename(f)
      print("DEBUG4 linking", f, dst_f)
      os.symlink(f, dst_f)
      

  # Executes a subgraph, calls appropriate executor as specified in nn_graph.json
  # Returns 0 on success, -1 or error code of failure.
  def run(self):
    executor = self.executor
    logFile = "log-exec-%s-%s.txt" % (self.sgDir, executor)
    self.logfile = logFile

    if executor == "host":
      cmd = "PATH=%s/runtime/util:$PATH runtime_tf" % kPath
      cmd += "  --tfpb %s"  % "%s/tf.pb" %(kelf_dir)
      inputs = []
      for input in self.getUpdatedInputs():
        inputs += [input["name"]]
        inputs += [input["file"]]

      cmd += "  --input_tensors %s" % " ".join(inputs)
      outputs = []
      for output in self.outputs:
        outputs += [output["name"], output["file"].replace(".npy", "-out.npy")]
      cmd += "  --output_tensors %s" % " ".join(outputs)
    elif executor == "tcc" or executor == "wave":
      self.linkConsts()
      self.createTpbFormatInputs()
      cmd = "PATH=%s/runtime/util:$PATH runtime_sim" % kPath
      cmd += "  --tpb_dir %s" % "%s/%s" %(kelf_dir, self.sgDir)
      if self.parallel_streams:
        cmd += " --parallel_streams"
      cmd += ' --inkling_debugflags %s' % " ".join(args.inkling_debugflags)
    elif 'qemu' in executor:
      self.linkConsts()
      ifmapFiles = self.createTpbFormatInputs()
      cmd  = "PATH=%s/runtime/util:$PATH qemu_rt" % kPath
      cmd += "  --action inference --kelf %s/%s" % (kelf_dir, self.sgDir)
      cmd += "  --sg %s" % self.sgDir
      cmd += "  --ifmaps %s" % " ".join(map(str, ifmapFiles))
      qemuPool = os.getenv("KAENA_QEMU_RT_POOL", None)
      qemuZebu = os.getenv("KAENA_ZEBU_SERVER", None)
      if not qemuZebu == None:
        cmd += '  --zebu "%s"' % qemuZebu
      elif not qemuPool == None:
        cmd += "  --pool %s" % qemuPool
      #cmd += ' --inkling_debugflags %s' % " ".join(args.inkling_debugflags)
    elif executor == "processor":
      inputs = []
      for input in self.getUpdatedInputs():
        inputs += [input["file"]]

      cmd = "%s/%s/%s" % (kelf_dir, self.sgDir, self.cmd)

      cmd += "  --inputs %s" % " ".join(inputs)
      if self.cmd_args:
        cmd += " " + self.cmd_args
      OutFile = ""
      if len(self.outputs) > 0:
        OutFile = self.outputs[0]["file"].replace(".npy", "-out.npy")
      if OutFile != "":
        cmd += "  --output %s" % (OutFile)
    elif executor == "waveopt":
      self.linkConsts()
      self.createTpbFormatInputs()
      cmd = "python3 %s/compiler/me/layeropt.py" % kPath
      cmd += "  --kgraph %s/%s/compiler.json" % (kelf_dir, self.sgDir)
      cmd += "  --inference"
    elif executor == "waveopt2":
      self.linkConsts()
      self.createTpbFormatInputs()
      cmd = "python3 %s/compiler/me/me_main.py" % kPath
      cmd += "  --kgraph %s/%s/compiler.json" % (kelf_dir, self.sgDir)
      cmd += "  --inference"
    cmd += "  " + self.execOptions
    cmd = "%s > %s 2>&1" % (cmd, logFile)
    print("\n", getInfoDateStr(), "executing %s" % cmd, flush=True)
    ret = os.system(cmd)
    if ret != 0:
      print("\nERROR: command  %s  returned non-zero exit code" % cmd, flush=True)
      return ret


    # Translate TPB format back to TF format.
    isWaveopt = (executor == "waveopt" or executor == "waveopt2")
    if (executor == "tcc" or  executor == "wave" or 'qemu' in executor or isWaveopt) and len(self.outputs) > 0:
      for jsonOutput in self.outputs:
        outNodeName, outFileRaw, outShape = [jsonOutput[k] for k in ['name', 'file', 'shape']]
        tff = self.tensorFormatMap.get(outNodeName)
        outFile = outFileRaw.replace(".npy", "-out.npy")
        outBase = outFileRaw[:-4]
        simExt = "simout"
        if isWaveopt:
          simExt = "midout"
        tpbFmapFormat = tff.simFormat
        tpbOutFile = "%s_%s-%s.npy" % (outBase, tpbFmapFormat, simExt)
        outFileQuemu = "%s_%s.bin" % (outBase, tpbFmapFormat)
        
        if 'qemu' in executor:
          # qemu_inkling runtime outputs raw binary portion of the ofmap in TPB format (not full npy)
          print("INFO: nn_executor: translating Qemu output %s to %s" % (outFileQuemu, tpbOutFile))
          if os.path.isfile(outFileQuemu):
            outFileOrig = "%s/%s/%s" % (kelf_dir, self.sgDir, tpbOutFile.replace("-simout.npy", ".npy"))
            ofmapOrig = np.load(outFileOrig)
            ofmapTpbShape = ofmapOrig.shape
            ofmapTpbDtype = ofmapOrig.dtype
            ofmapQuemu = np.fromfile(outFileQuemu, dtype=ofmapTpbDtype).reshape(ofmapTpbShape)
            np.save(tpbOutFile, ofmapQuemu)
          else:
            print("ERROR: nn_executor: missing Quemu Inkling output file %s" % outFileQuemu)
        
        if os.path.isfile(tpbOutFile):
          outShape = self.outputs[0]["shape"]
          npt.formatNpyFileAs(tpbOutFile,  tff.simFormat, tff.tfFormat, dstShape=outShape, outFile=outFile)
        else:
          print("ERROR: no tpb output file %s / %s" % (self.sgDir, tpbOutFile), flush=True)
          return -1

    return 0
    

class NnExec:
  def __init__(self, nnGraphFile):
    with open(nnGraphFile) as fh:
      self.nnGraphJsonData = json.load(fh)
    self.subGraphs = []

  def update_io(self, input_files, output_files):
    sg_first = self.subGraphs[0]
    sg_last = self.subGraphs[-1]

    # Attach supplied input file to first subgraph
    if len(input_files) != 0:
      if len(sg_first.inputs) == 0:
        # Preprocessor case (e.g. batching)
        for i in range(len(input_files)):
          if i not in input_files:
            print("ERROR: Input node names specified for graph with no named inputs.")
            sys.exit(1)
          else:
            sg_first.inputs.append({"name": "", "file": input_files[i]})
      else:
        assert (len(sg_first.inputs) == len(input_files))
        for i in range(len(sg_first.inputs)):
          # Try to find input by name
          if sg_first.inputs[i]["name"] in input_files:
            sg_first.inputs[i]["file"] = input_files[sg_first.inputs[i]["name"]]
          # If it's the only input, we also allow just the file name
          elif i == 0 and len(sg_first.inputs) == 1 and i in input_files:
            sg_first.inputs[i]["file"] = input_files[i]
          else:
            print("ERROR: Could not find input file for node %s" % (sg_first.inputs[i]["name"]))
            sys.exit(1)

    # Attach supplied output file to last subgraph
    if len(output_files) != 0:
      if len(sg_last.outputs) == 0:
        sg_last.inputs.append({"name" : "", "file" : output_files[0]})
      else:
        assert (len(sg_last.outputs) == len(output_files))
        # we only support single output file for now
        sg_last.outputs[0]["file"] = output_files[0]

  def run(self):

    sgId = 0
    for sgJson in self.nnGraphJsonData["SubGraphs"]:
      sg = NnSubgraph(sgId, sgJson)
      self.subGraphs.append(sg)
      sgId += 1

    # copy input files to local working directory
    for f in input_files.values():
      shutil.copy(f, working_dir)

    self.update_io(input_files, output_files)

    for sg in self.subGraphs:
      ret = sg.run()
      if (ret != 0):
        return ret
    return 0


  # Return unix exit code. Subgraph mode can be "last" or "all*"
  #   all_available   includes tensors for which qemu output bin is present.
  #   all_internal    includes all non-constant tensors in the subgraph
  # The cachedGoldens 
  #     additional comparison is done against all "new" (computed) tensor files
  #     This is useful in eg running qemu, then rerun with --cached_kelf on zebu
  def check(self, sgMode, diffOptions, cachedGoldensPath, diffOptionsCached):
    # find last non processor subgraph, compare output.
    sgList = []
    if sgMode == 'last':
      lastSg = None
      for sg in self.subGraphs:
        if sg.executor != "processor":
          lastSg = sg
      sgList = [lastSg]
    elif sgMode.startswith('all'):
      sgList = self.subGraphs
    else:
      raise ValueError("ERROR: Unsupported subgraph mode --check_against_ref '%s'" % sgMode)
    comparedFiles = []
    for sg in sgList:
      print("\nINFO: nn_executor.check subgraph  %s OUTPUT tensors" % (sg.sgDir), flush=True)
      retAll = 0
      for jsonOutput in sg.outputs:
        refOutFile = jsonOutput['file']
        outBase = refOutFile[:-4]
        flowOutFile = outBase + "-out.npy"
        comparedFiles.append(flowOutFile)
        cmd = "%s/compiler/util/npy_diff_files %s --gold %s/%s --new %s" % (
                kPath, diffOptions, kelf_dir, refOutFile, flowOutFile)
        print("INFO: executing %s" % cmd, flush=True)
        ret = os.system(cmd)
        retAll = ret | retAll
        print("INFO: subgraph  %s  output tensor  %s  status  %s" % (
                sg.sgDir, jsonOutput['name'], ('PASS' if ret == 0 else 'FAIL')), flush=True)
      if 'wave' in sg.executor and sgMode.startswith('all_'):
        print("\nINFO: nn_executor.check subgraph  %s INTERNAL tensors" % (sg.sgDir), flush=True)
        # Get all internal tensors
        for tensorName, tff in sg.tensorFormatMap.getNonConstTensors():
          print("\nINFO: comparing internal tensor ", tensorName)
          # All files are in Tonga format
          refFile = '%s/%s/%s' % (kelf_dir, sg.sgDir, tff.simFile)
          newFile = tff.simFile
          if 'qemu' in sg.executor:
            newFile = newFile.replace('.npy', '.bin')
          else:
            newFile = newFile.replace('.npy', '-simout.npy')
          if not os.path.isfile(newFile):
            if sgMode == 'all_available':
              print('WARNING: --check_against_ref all_available  ignoring missing file ', newFile)
            else:
              print('ERROR: --check_against_ref all_internal  missing file ', newFile)
              retAll = 1
          else:
            cmd = "%s/compiler/util/npy_diff_files %s --gold %s --new %s" % (
                    kPath, diffOptions, refFile, newFile)
            comparedFiles.append(newFile)
            print("INFO: executing %s" % cmd, flush=True)
            ret = os.system(cmd)
            retAll = ret | retAll
            print("INFO: subgraph  %s  internal tensor  %s  status  %s" % (
                    sg.sgDir, tensorName, ('PASS' if ret == 0 else 'FAIL')), flush=True)
      if retAll != 0:
        return retAll
    
    if cachedGoldensPath:
      print("\nINFO: nn_executor.check Comparing %d cached golden tensors" % (len(comparedFiles)), flush=True)
      for f in comparedFiles:
        refFile = "%s/%s" % (cachedGoldensPath, f)
        newFile = f
        cmd = "%s/compiler/util/npy_diff_files %s --gold %s --new %s" % (
                kPath, diffOptionsCached, refFile, newFile)
        print("INFO: executing %s" % cmd, flush=True)
        ret = os.system(cmd)
        retAll = ret | retAll
      if retAll != 0:
        return retAll
    
    return 0

ret = 0
os.chdir(working_dir)
nnExec = NnExec(nnGraphFile)
ret = nnExec.run()
if ret != 0:
  raise RuntimeError("ERROR: nn_executor has returned non-zero exit status, please check log-exec*.txt")
if (ret == 0) and (args.check_against_ref != "none"):
  ret = nnExec.check(args.check_against_ref, args.diff_options,
                     args.cached_goldens, args.diff_options_cached)

print(getInfoDateStr(), "Kaena RT status %s" % ("PASS" if ret == 0 else "FAIL"))
sys.exit(0 if ret == 0 else 1)



