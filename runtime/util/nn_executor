#!/usr/bin/env python3

# Copyright (C) 2018, Amazon.com. All Rights Reserved
#
# Kaena neural network executor with mapping to mixed backends.
#

import argparse
import os.path
import sys, json, re
import glob
import shutil
kPath = os.environ.get('KAENA_PATH')
if kPath is None:
  kPath =''
sys.path.insert(0, kPath + "/compiler/")
from tffe.NpTransforms import NpTrans as npt

print("\nINFO: started as  ", " ".join(sys.argv), flush=True)

parser = argparse.ArgumentParser()
parser.add_argument('--kelf_dir', help='kelf directory', default="")
parser.add_argument('--tfpb', help='TensorFlow freeze graph file', default="f.pb")
parser.add_argument('--input_files', help='Files to be passed in to the first subgraph', default=[], nargs='+')
parser.add_argument('--output_files', help='Specify output files for last subgraph', default=[], nargs='+')
parser.add_argument('--check_against_ref', help='compare last produced outputs npy file against compiler provided goldens', action='store_true')
parser.add_argument('--working_dir', help='all output will be stored here', default="")
parser.add_argument('--inkling_debugflags', help='inkling extra args', default=[], nargs='*')

args = parser.parse_args()
kelf_dir = args.kelf_dir
kelf_dir = os.path.abspath(kelf_dir)
assert(kelf_dir != "")
nnGraphFile = kelf_dir + "/nn_graph.json"

input_files = [os.path.abspath(f) for f in args.input_files]
assert(1 >= len(input_files))
output_files = [os.path.abspath(f) for f in args.output_files]
working_dir = args.working_dir
if (working_dir == ""):
  working_dir = "%s/working_dir" % kelf_dir
working_dir = os.path.abspath(working_dir)
if not os.path.exists(working_dir):
  os.makedirs(working_dir)

print("INFO: using working directory  %s" % (working_dir))



class NnSubgraph:
  def __init__(self, sgId, sgJson):
    self.sgId = sgId
    self.sgDir = sgJson["SubGraphDir"]
    self.inputs = sgJson["Inputs"]
    self.outputs = sgJson["Outputs"]
    self.executor = sgJson["executor"]
    self.parallel_streams = sgJson.get("parallel_streams", False)
    self.cmd = ""
    if self.executor == "processor":
      self.cmd = sgJson["cmd"]

  # Translate and relink npy files that are inputs to this subgraph
  # Each subgraph produces output in the host format. So just rename npy
  # files A:0.npy to point to ../A:0-out.npy
  def getUpdatedInputs(self):
    newInputs = []
    if self.sgId == 0:
      return self.inputs

    for input in self.inputs:
      name = input["name"]
      f = input["file"]
      tfFile = "%s" % f.replace(".npy", "-out.npy")
      newInputs += [{"name" : name, "file" :tfFile}]

    return newInputs


  def createTpbFormatInputs(self):
    if self.sgId == 0:
      # The inputs for the first subgraph are same as when compiled
      return
    for input in self.inputs:
      name = input["name"]
      f = input["file"]
      fBase = f[:-4]  # removes .npy
      # Only Fmaps come from the previous subgraph
      tpbInpFile = "%s_%s.npy"      % (fBase, npt.Formats[npt.SIM][npt.Fmaps])
      tfFile = "%s-out.npy" % fBase

      if os.path.isfile(tpbInpFile):
        # Move the compiler generated input file
        origFile = tpbInpFile.replace(".npy", "-orig.npy")
        os.rename(tpbInpFile, origFile)

      # Translate and write the previous subgraph file
      if os.path.isfile(tfFile):
        inputShapeTf = input["shape"]
        # FIX_THIS: longterm we should store both TF and TBP IO formats in the nn_graph.json
        tfShapes = [None, npt.C, npt.NC, npt.HNC, npt.Formats[npt.TF][npt.Fmaps]]
        inputFormatTf = tfShapes[len(input['shape'])]
        npt.formatNpyFileAs(tfFile, inputFormatTf, npt.Formats[npt.SIM][npt.Fmaps], outFile=tpbInpFile)
        print("INFO: translated TF format  %s  to TPB Fmap format  %s" % (tfFile, tpbInpFile))
      else:
        print("ERROR: no tf file not found %s" % tfFile, flush=True)
        sys.exit(-1)

  # There is no good way to identify consts (weights, biases, ..) in the kelf file yet.
  # This is a hack to link required files so that SIM can find them.
  def linkConsts(self):
    g = []
    g += glob.glob("%s/*%s*.npy"% (working_dir, npt.Formats[npt.SIM][npt.Weights]))
    g += glob.glob("%s/*%s*.npy"% (working_dir, npt.Formats[npt.SIM][npt.Fmaps]))
    for f in g:
      os.remove(f)
    g = []
    g += glob.glob("%s/sg*/*%s*.npy"% (kelf_dir, npt.Formats[npt.SIM][npt.Weights]))
    g += glob.glob("%s/sg*/*%s*.npy"% (kelf_dir, npt.Formats[npt.SIM][npt.Fmaps]))
    for f in g:
      dst_f = "%s/%s" % (working_dir, os.path.basename(f))
      if not os.path.isfile(dst_f) and not "out.npy" in f:
          os.symlink(f, dst_f )


  # Executes a subgraph, calls appropriate executor as specified in nn_graph.json
  # Returns 0 on success, -1 or error code of failure.
  def run(self):
    executor = self.executor
    logFile = "log-exec-%s-%s.txt" % (self.sgDir, executor)

    if executor == "host":
      cmd = "PATH=%s/runtime/util:$PATH runtime_tf" % kPath
      cmd += "  --tfpb %s"  % "%s/tf.pb" %(kelf_dir)
      inputs = []
      for input in self.getUpdatedInputs():
        inputs += [input["name"]]
        inputs += [input["file"]]

      cmd += "  --input_tensors %s" % " ".join(inputs)
      outputs = []
      for output in self.outputs:
        outputs += [output["name"], output["file"].replace(".npy", "-out.npy")]
      cmd += "  --output_tensors %s" % " ".join(outputs)
    elif executor == "tcc" or executor == "wave":
      self.linkConsts()
      self.createTpbFormatInputs()
      cmd = "PATH=%s/runtime/util:$PATH runtime_sim" % kPath
      cmd += "  --tpb_dir %s" % "%s/%s" %(kelf_dir, self.sgDir)
      if self.parallel_streams:
        cmd += " --parallel_streams"
      cmd += ' --inkling_debugflags %s' % " ".join(args.inkling_debugflags)
    elif executor == "processor":
      inputs = []
      for input in self.getUpdatedInputs():
        inputs += [input["file"]]

      cmd = "%s/%s/%s" % (kelf_dir, self.sgDir, self.cmd)

      cmd += "  --inputs %s" % " ".join(inputs)
      OutFile = ""
      if len(self.outputs) > 0:
        OutFile = self.outputs[0]["file"].replace(".npy", "-out.npy")
      if OutFile != "":
        cmd += "  --output %s" % (OutFile)
    elif executor == "waveopt":
      self.linkConsts()
      self.createTpbFormatInputs()
      cmd = "python3 %s//compiler/util/layeropt.py" % kPath
      cmd += "  --kgraph %s/%s/compiler.json" % (kelf_dir, self.sgDir)
      cmd += "  --inference"
    cmd = "%s > %s 2>&1" % (cmd, logFile)
    print("\nINFO: executing %s" % cmd, flush=True)
    ret = os.system(cmd)
    if ret != 0:
      return ret


    # Translate TPB format back to TF format.
    if (executor == "tcc" or  executor == "wave" or executor == "waveopt") and len(self.outputs) > 0:
      for jsonOutput in self.outputs:
        outNodeName, outFileRaw, outShape = [jsonOutput[k] for k in ['name', 'file', 'shape']]
        outFile = outFileRaw.replace(".npy", "-out.npy")
        outBase = outFileRaw[:-4]
        simExt = "simout"
        if executor == "waveopt":
          simExt = "midout"
        tpbFmapFormat = npt.Formats[npt.SIM][npt.Fmaps]
        tpbOutFile = "%s_%s-%s.npy" % (outBase, tpbFmapFormat, simExt)
        if os.path.isfile(tpbOutFile):
          outShape = self.outputs[0]["shape"]
          npt.copyNpyFileAs(tpbOutFile, npt.SIM, npt.TF, npt.Fmaps, outFile=outFile, dstShape=outShape)
        else:
          print("ERROR: no tpb output file %s" % tpbOutFile, flush=True)
          return -1

    return 0
    

class NnExec:
  def __init__(self, nnGraphFile):
    with open(nnGraphFile) as fh:
      self.nnGraphJsonData = json.load(fh)
    self.subGraphs = []

  def update_io(self, input_files, output_files):
    sg_first = self.subGraphs[0]
    sg_last = self.subGraphs[-1]

    # Attach supplied input file to first subgraph
    if len(input_files) != 0:
      if len(sg_first.inputs) == 0:
        sg_first.inputs.append({"name" : "", "file" : input_files[0]})
      else:
        assert (len(sg_first.inputs) == len(input_files))
        # we only support single input file for now
        sg_first.inputs[0]["file"] = input_files[0]

    # Attach supplied output file to last subgraph
    if len(output_files) != 0:
      if len(sg_last.outputs) == 0:
        sg_last.inputs.append({"name" : "", "file" : output_files[0]})
      else:
        assert (len(sg_last.outputs) == len(output_files))
        # we only support single output file for now
        sg_last.outputs[0]["file"] = output_files[0]

  def run(self):

    sgId = 0
    for sgJson in self.nnGraphJsonData["SubGraphs"]:
      sg = NnSubgraph(sgId, sgJson)
      self.subGraphs.append(sg)
      sgId += 1

    # copy input files to local working directory
    for f in input_files:
      shutil.copy(f, working_dir)

    self.update_io(input_files, output_files)

    for sg in self.subGraphs:
      ret = sg.run()
      if (ret != 0):
        return ret
    return 0


  # Return unix exit code
  def check(self):
    # find last non processor subgraph, compare output.
    lastSg = None
    for sg in self.subGraphs:
      if sg.executor != "processor":
        lastSg = sg
    ret = 0
    for jsonOutput in lastSg.outputs:
      refOutFile = jsonOutput['file']
      outBase = refOutFile[:-4]
      flowOutFile = outBase + "-out.npy"
      cmd = "%s/compiler/util/npy_diff_files --gold %s/%s --new %s" % (kPath, kelf_dir, refOutFile, flowOutFile)
      print("INFO: executing %s" % cmd, flush=True)
      if ret == 0:
        ret = os.system(cmd)
    return ret

ret = 0
os.chdir(working_dir)
nnExec = NnExec(nnGraphFile)
ret = nnExec.run()
assert(ret == 0)
if args.check_against_ref:
  ret = nnExec.check()
  assert(ret == 0)

print("INFO: Kaena RT status %s" % ("PASS" if ret == 0 else "FAIL"))
sys.exit(0 if ret == 0 else 1)



