#!/usr/bin/env python3

# Copyright (C) 2018, Amazon.com. All Rights Reserved
#
# Kaena neural network executor with mapping to mixed backends.
#

import argparse
import os.path
import sys, json, re, getpass, random
import glob
import shutil
import socket, errno
import datetime
kPath = os.environ.get('KAENA_PATH')
if kPath is None:
  kPath =''
sys.path.insert(0, kPath + "/compiler")
aws_tonga_src_path = os.environ.get('AWS_TONGA_SRC')

sys.path.insert(0, "/usr/lib/python3.6/site-packages/")
import numpy as np
from tffe.NpTransforms import NpTrans as npt
from tffe.NpTransforms import TensorFormat, TensorFormatMap
from me.me_utils import pad_and_split_file

def getInfoDateStr():
  return "INFO %s :" % str(datetime.datetime.now())


print("\n", getInfoDateStr(), " started as  ", " ".join(sys.argv), flush=True)

parser = argparse.ArgumentParser()
parser.add_argument('--kelf_dir', help='kelf directory', default="")
parser.add_argument('--tfpb', help='TensorFlow freeze graph file', default="f.pb")
parser.add_argument('--input_files', help='Files to be passed in to the first subgraph (filenames or node=filename pairs)', default=[], nargs='+')
parser.add_argument('--output_files', help='Specify output files for last subgraph', default=[], nargs='+')
parser.add_argument('--check_against_ref', help='Compare produced subgraph output npy files against compiler provided goldens in local directory; use none (default), last, all, all_available, all_internal, only_golden', default='none')
parser.add_argument('--working_dir', help='all output will be stored here', default="")
parser.add_argument('--inkling_debugflags', help='inkling extra args', default=[], nargs='*')
parser.add_argument('--env', help='Set envvars pairs var=val var1=val1. Used to pass experimental controls to low-level tools', nargs='+', default=[])
parser.add_argument('--cached_goldens', help='Path to a cached runs. New files created (and diffed against TF) in this run are also compared against the new file in the cached run', default=None)
parser.add_argument('--diff_options', help='String passed to npy_diff_files utility, such as to control tolerance, default is ""', default="")
parser.add_argument('--diff_options_cached', help='As diff options, but used for --cached_goldens. Default is "--binary_match"', default='--binary_match')

args = parser.parse_args()
kelf_dir = args.kelf_dir
kelf_dir = os.path.abspath(kelf_dir)
assert(kelf_dir != "")
nnGraphFile = kelf_dir + "/nn_graph.json"

# The --env support
if len(args.env) > 0:
  for varEqVal in args.env:
    var, val = varEqVal.split("=")
    os.environ[var] = val

def parseInputFilesArg(arg, input_files):
  for i in range(len(args.input_files)):
    if '=' in args.input_files[i]:
      (node, fname) = args.input_files[i].split("=", 1)
    else:
      node = i
      fname = args.input_files[i]
    input_files[node] = os.path.abspath(fname)

input_files = {}
parseInputFilesArg(args.input_files, input_files)

output_files = [os.path.abspath(f) for f in args.output_files]
working_dir = args.working_dir
if (working_dir == ""):
  working_dir = "%s/working_dir" % kelf_dir
working_dir = os.path.abspath(working_dir)

if os.path.exists(working_dir):
  shutil.rmtree(working_dir)
os.makedirs(working_dir)

print(getInfoDateStr(), " using working directory  %s" % (working_dir))

class NnSubgraph:
  def __init__(self, sgId, sgJson):
    self.sgId = sgId
    self.sgDir = sgJson["SubGraphDir"]
    self.inputs = sgJson["Inputs"]
    self.outputs = sgJson["Outputs"]
    self.executor = sgJson["executor"]
    self.execOptions = sgJson.get("execOptions", "")
    self.parallel_streams = sgJson.get("parallel_streams", False)
    self.tensorFormatMap = TensorFormatMap()
    self.cmd = ""
    self.cmd_args = ""
    if self.executor == "processor":
      self.cmd = sgJson["cmd"]
      self.cmd_args = sgJson["cmd_args"]
    if re.search(r'wave', self.executor):
      self.tensorFormatMap.readJson("%s/%s/tensor_map.json" % (kelf_dir, self.sgDir))

  # Translate and relink npy files that are inputs to this subgraph
  # Each subgraph produces output in the host format. So just rename npy
  # files A:0.npy to point to ../A:0-out.npy
  def getUpdatedInputs(self):
    newInputs = []
    if self.sgId == 0:
      return self.inputs

    for input in self.inputs:
      name = input["name"]
      f = input["file"]
      tfFile = "%s" % f.replace(".npy", "-out.npy")
      newInputs += [{"name" : name, "file" :tfFile}]

    return newInputs

  # Returns input file names in TPB format
  def createTpbFormatInputs(self):
    inputFilesInTpbFotmat = []
    for input in self.inputs:
      name = input["name"]
      f = input["file"]
      tff = self.tensorFormatMap.get(name)
      assert tff != None
      tpbInpFile = tff.simFile
      if self.sgId == 0:
        # The inputs for the first subgraph are same as when compiled
        tfFile = f
      else:
        assert f == tff.tfFile
        # For next sungraph it is teh output of previous subgraph (hence -out.npy)
        tfFile = f.replace(".npy", "-out.npy")

      if os.path.isfile(tpbInpFile):
        # Move the compiler generated input file
        origFile = tpbInpFile.replace(".npy", "-orig.npy")
        os.rename(tpbInpFile, origFile)

      # Translate and write the previous subgraph file
      if os.path.isfile(tfFile):
        inputShapeTf = input["shape"]
        inputFormatTf = tff.tfFormat
        inputFormatSim = tff.simFormat
        
        npt.formatNpyFileAs(tfFile, inputFormatTf, inputFormatSim, outFile=tpbInpFile, srcShape=input['shape'])
        print(getInfoDateStr(), "INFO: nn_executor sg%02d:  translated TF format %s %s  to TPB Fmap format %s %s" % (self.sgId, inputFormatTf, tfFile, inputFormatSim, tpbInpFile))
        assert os.path.isfile(tpbInpFile)  # exists from compilation
        # The inputs for the first subgraph are same as when compiled
        ## # kaena-602: improved for inference by extracting pad/split parameters and apply to inference input
        tpbInpPadSplitFileList = glob.glob(tpbInpFile[:-4] + "_padsplit_*.npy")
        #if os.path.isfile(tpbInpPadSplitFile):
        if len(tpbInpPadSplitFileList) > 0:
          if len(tpbInpPadSplitFileList) > 1:
              print("WARNING: more than one pad/split file found; using the first one to obtain pad/split parameters.")
          assert(inputFormatSim == "NCHW" or inputFormatSim == "NHWC")
          m = re.search(r"padsplit_stride(\d)_n(\d)_s(\d)_w(\d)_e(\d)_pc(\d)", tpbInpPadSplitFileList[0])
          if m:
              stride = int(m.group(1))
              padN = int(m.group(2))
              padS = int(m.group(3))
              padW = int(m.group(4))
              padE = int(m.group(5))
              padConst = int(m.group(6))
          else:
              raise RuntimeError("Cannot extract pad/split parameters from file name %s"%tpbInpPadSplitFileList[0])
          cmd = "%s/compiler/util/padsplit_npy_for_repl.py --input_file %s --format %s --stride %d --padding=\"[ [0,0], [0,0], [%d,%d], [%d,%d] ]\" --pad_const %d" % (
                  kPath, tpbInpFile, inputFormatSim, stride, padN, padS, padW, padE, padConst)
          print("INFO: executing %s" % cmd, flush=True)
          ret = os.system(cmd)
          if ret != 0:
            print("\nERROR: command  %s  returned non-zero exit code" % cmd, flush=True)
            sys.exit(-1)
          tpbInpFile = tpbInpPadSplitFileList[0]
          assert os.path.isfile(tpbInpFile)
        inputFilesInTpbFotmat.append(tpbInpFile)
      else:
        print("ERROR: no tf file not found %s" % tfFile, flush=True)
        sys.exit(-1)
    return inputFilesInTpbFotmat

  # Constant npy files like weights are crated by compiler (tffe)
  # in sg* and thus linked for running inference (here, which is <test>/working_dir)
  def linkConsts(self):
    for f in self.tensorFormatMap.getConstFilesSim():
      src_f = "%s/%s/%s" % (kelf_dir, self.sgDir, f)
      print("DEBUG3 linking", src_f, f)
      os.symlink(src_f, f)
    # ME creates files of constants that are not visible to TFFE, so link them too
    files =  glob.glob("../%s/*-constants.npy" % self.sgDir)
    files += glob.glob("../%s/*_padsplit*.npy" % self.sgDir)
    files += glob.glob("../%s/*_concat_weight*.npy" % self.sgDir)
    for f in files:
      dst_f = os.path.basename(f)
      print("DEBUG4 linking", f, dst_f)
      if os.path.isfile(dst_f):
          os.remove(dst_f)
      os.symlink(f, dst_f)

  # Executes a subgraph, calls appropriate executor as specified in nn_graph.json
  # Returns 0 on success, -1 or error code of failure.
  def run(self):
    executor = self.executor
    logFile = "log-exec-%s-%s.txt" % (self.sgDir, executor)
    self.logfile = logFile

    if executor == "host":
      cmd = "PATH=%s/runtime/util:$PATH runtime_tf" % kPath
      cmd += "  --tfpb %s"  % "%s/tf.pb" %(kelf_dir)
      inputs = []
      for input in self.getUpdatedInputs():
        inputs += [input["name"]]
        inputs += [input["file"]]

      cmd += "  --input_tensors %s" % " ".join(inputs)
      outputs = []
      for output in self.outputs:
        outputs += [output["name"], output["file"].replace(".npy", "-out.npy")]
      cmd += "  --output_tensors %s" % " ".join(outputs)
    elif executor == "tcc" or executor == "wave":
      self.linkConsts()
      self.createTpbFormatInputs()
      cmd = "PATH=%s/runtime/util:$PATH runtime_sim" % kPath
      cmd += "  --tpb_dir %s" % "%s/%s" %(kelf_dir, self.sgDir)
      if self.parallel_streams:
        cmd += " --parallel_streams"
      cmd += ' --inkling_debugflags %s' % " ".join(args.inkling_debugflags)
    elif executor == 'wavesim':
      self.linkConsts()
      self.createTpbFormatInputs()
      cmd = "%s/starfish/bin/wavegrind --mode simulation ../%s/wavegraph.json --ref=none --cwd=." % (aws_tonga_src_path, self.sgDir)
    elif 'qemu' in executor:
      self.linkConsts()
      ifmapFiles = self.createTpbFormatInputs()
      cmd  = "PATH=%s/runtime/util:$PATH qemu_rt" % kPath
      cmd += "  --action inference --kelf %s/%s" % (kelf_dir, self.sgDir)
      cmd += "  --sg %s" % self.sgDir
      cmd += "  --ifmaps %s" % " ".join(map(str, ifmapFiles))
      qemuPool = os.getenv("KAENA_QEMU_RT_POOL", None)
      qemuZebu = os.getenv("KAENA_ZEBU_SERVER", None)
      if not qemuZebu == None:
        cmd += '  --zebu "%s"' % qemuZebu
      elif not qemuPool == None:
        cmd += "  --pool %s" % qemuPool
      #cmd += ' --inkling_debugflags %s' % " ".join(args.inkling_debugflags)
    elif executor == "processor":
      inputs = []
      for input in self.getUpdatedInputs():
        inputs += [input["file"]]

      cmd = "%s/%s/%s" % (kelf_dir, self.sgDir, self.cmd)

      cmd += "  --inputs %s" % " ".join(inputs)
      if self.cmd_args:
        cmd += " " + self.cmd_args
      OutFiles = []
      if len(self.outputs) > 0:
        for i in range(len(self.outputs)):
          OutFiles.append(self.outputs[0]["file"].replace(".npy", "-out.npy"))
      if len(OutFiles) > 0:
        cmd += "  --output %s" % (" ".join(OutFiles))
    elif executor == "waveopt":
      self.linkConsts()
      self.createTpbFormatInputs()
      cmd = "python3 %s/compiler/me/layeropt.py" % kPath
      cmd += "  --kgraph %s/%s/compiler.json" % (kelf_dir, self.sgDir)
      cmd += "  --inference"
    elif executor == "waveopt2":
      self.linkConsts()
      self.createTpbFormatInputs()
      cmd = "python3 %s/compiler/me/me_main.py" % kPath
      cmd += "  --kgraph %s/%s/compiler.json" % (kelf_dir, self.sgDir)
      cmd += "  --inference"
    cmd += "  " + self.execOptions
    cmd = "%s > %s 2>&1" % (cmd, logFile)
    print("\n", getInfoDateStr(), " executing %s" % cmd, flush=True)
    ret = os.system(cmd)
    if ret != 0:
      print("\nERROR: command  %s  returned non-zero exit code %d" % (cmd, ret), flush=True)
      return ret
    else:
      print("Info: command %s finished" % cmd)


    # Translate TPB format back to TF format.
    isWaveopt = (executor == "waveopt" or executor == "waveopt2")
    isQemu = 'qemu' in executor
    if (executor == "tcc" or  executor == "wave" or executor == "wavesim" or isQemu or isWaveopt) and len(self.outputs) > 0:
      for jsonOutput in self.outputs:
        outNodeName, outFileRaw, outShape = [jsonOutput[k] for k in ['name', 'file', 'shape']]
        tff = self.tensorFormatMap.get(outNodeName)
        if tff == None:
          raise ValueError("ERROR: tensor map for output %s was not found in tensor_map.json "
                           "Correct operator suppot in genCompilerLayerJson() and rerun" % outNodeName)
        outFile = outFileRaw.replace(".npy", "-out.npy")
        outBase = outFileRaw[:-4]
        simExt = "simout"
        if isWaveopt:
          simExt = "midout"
        tpbFmapFormat = tff.simFormat
        tpbOutFile = "%s_%s-%s.npy" % (outBase, tpbFmapFormat, simExt)
        outFileQuemu = "%s_%s.bin" % (outBase, tpbFmapFormat)
        
        if 'qemu' in executor:
          # qemu_inkling runtime outputs raw binary portion of the ofmap in TPB format (not full npy)
          print("INFO: nn_executor: translating Qemu output %s to %s" % (outFileQuemu, tpbOutFile))
          if os.path.isfile(outFileQuemu):
            outFileOrig = "%s/%s/%s" % (kelf_dir, self.sgDir, tpbOutFile.replace("-simout.npy", ".npy"))
            ofmapOrig = np.load(outFileOrig)
            ofmapTpbShape = ofmapOrig.shape
            ofmapTpbDtype = ofmapOrig.dtype
            ofmapQuemu = np.fromfile(outFileQuemu, dtype=ofmapTpbDtype).reshape(ofmapTpbShape)
            np.save(tpbOutFile, ofmapQuemu)
          else:
            print("ERROR: nn_executor: missing Quemu Inkling output file %s" % outFileQuemu)
        
        if os.path.isfile(tpbOutFile):
          outShape = jsonOutput["shape"]
          npt.formatNpyFileAs(tpbOutFile,  tff.simFormat, tff.tfFormat, dstShape=outShape, outFile=outFile)
        else:
          print("ERROR: no tpb output file %s / %s" % (self.sgDir, tpbOutFile), flush=True)
          return -1
      if isQemu:
        # Create PE utilization profile
        testName = os.getcwd().split('/')[-2]
        profileOut = 'tpb_profile-%s.png' % self.sgDir
        cmd = "%s/compiler/util/tpb_profile --log %s --dma_json dma_trace.json --tpb %s --resolution 2000 --title %s --output_files %s" % (
                kPath, 'trace_notifications.json', '*_final.bin',
                testName + '-' + self.sgDir, profileOut)
        print(getInfoDateStr(), " executing %s" % cmd, flush=True)
        ret = os.system(cmd)
        if ret == 0:
          if not os.path.isfile(profileOut):
            ret = 3
        if ret != 0:
          return ret
        
    return 0
    

class NnExec:
  def __init__(self, nnGraphFile):
    with open(nnGraphFile) as fh:
      self.nnGraphJsonData = json.load(fh)
    self.subGraphs = []

  def update_io(self, input_files, output_files):
    sg_first = self.subGraphs[0]
    sg_last = self.subGraphs[-1]

    # Attach supplied input file to first subgraph
    if len(input_files) != 0:
      if len(sg_first.inputs) == 0:
        # Preprocessor case (e.g. batching)
        for i in range(len(input_files)):
          if i not in input_files:
            print("ERROR: Input node names specified for graph with no named inputs.")
            sys.exit(1)
          else:
            sg_first.inputs.append({"name": "", "file": input_files[i]})
      else:
        assert (len(sg_first.inputs) == len(input_files))
        for i in range(len(sg_first.inputs)):
          # Try to find input by name
          if sg_first.inputs[i]["name"] in input_files:
            sg_first.inputs[i]["file"] = input_files[sg_first.inputs[i]["name"]]
          # If it's the only input, we also allow just the file name
          elif i == 0 and len(sg_first.inputs) == 1 and i in input_files:
            sg_first.inputs[i]["file"] = input_files[i]
          else:
            print("ERROR: Could not find input file for node %s" % (sg_first.inputs[i]["name"]))
            sys.exit(1)

    # Attach supplied output file to last subgraph
    if len(output_files) != 0:
      if len(sg_last.outputs) == 0:
        assert len(output_files) == 1
        sg_last.inputs.append({"name" : "", "file" : output_files[0]})
      else:
        assert (len(sg_last.outputs) == len(output_files))
        # we only support single output file for now
        for i in range(len(sg_last.outputs)):
          sg_last.outputs[i]["file"] = output_files[i]

  def run(self):

    sgId = 0
    for sgJson in self.nnGraphJsonData["SubGraphs"]:
      sg = NnSubgraph(sgId, sgJson)
      self.subGraphs.append(sg)
      sgId += 1

    # copy input files to local working directory
    for f in input_files.values():
      shutil.copy(f, working_dir)

    self.update_io(input_files, output_files)

    for sg in self.subGraphs:
      ret = sg.run()
      if (ret != 0):
        return ret
    return 0


  # Return unix exit code. Subgraph mode can be "last" or "all*"
  #   all_available   includes tensors for which qemu output bin is present.
  #   all_internal    includes all non-constant tensors in the subgraph
  # The cachedGoldens 
  #     additional comparison is done against all "new" (computed) tensor files
  #     This is useful in eg running qemu, then rerun with --cached_kelf on zebu
  def check(self, sgMode, diffOptions, cachedGoldensPath, diffOptionsCached):
    # find last non processor subgraph, compare output.
    sgList = []
    if sgMode == 'last':
      lastSg = None
      for sg in self.subGraphs:
        if sg.executor != "processor":
          lastSg = sg
      sgList = [lastSg]
    elif sgMode.startswith('all'):
      sgList = self.subGraphs
    elif sgMode == "only_golden":
      if cachedGoldensPath is None:
        print("INFO: No cached goldens path specified for only_goldens comparison. Skipping.")
        return 0
      assert os.path.isdir(cachedGoldensPath), \
          "cachedGoldensPath %s is not a directory" % (cachedGoldensPath)
      sgList = []
    else:
      raise ValueError("ERROR: Unsupported subgraph mode --check_against_ref '%s'" % sgMode)
    comparedFiles = []
    retAll = 0
    for sg in sgList:
      print("\nINFO: nn_executor.check subgraph  %s OUTPUT tensors" % (sg.sgDir), flush=True)
      for jsonOutput in sg.outputs:
        refOutFile = jsonOutput['file']
        outBase = refOutFile[:-4]
        flowOutFile = outBase + "-out.npy"
        comparedFiles.append(flowOutFile)
        cmd = "%s/compiler/util/npy_diff_files %s --gold %s/%s --new %s" % (
                kPath, diffOptions, kelf_dir, refOutFile, flowOutFile)
        print("INFO: executing %s" % cmd, flush=True)
        ret = os.system(cmd)
        retAll = ret | retAll
        print("INFO: subgraph  %s  output tensor  %s  status  %s" % (
                sg.sgDir, jsonOutput['name'], ('PASS' if ret == 0 else 'FAIL')), flush=True)
      if 'wave' in sg.executor and sgMode.startswith('all_'):
        print("\nINFO: nn_executor.check subgraph  %s INTERNAL tensors" % (sg.sgDir), flush=True)
        # Get all internal tensors
        for tensorName, tff in sg.tensorFormatMap.getNonConstTensors():
          print("\nINFO: comparing internal tensor ", tensorName)
          # All files are in Tonga format
          refFile = '%s/%s/%s' % (kelf_dir, sg.sgDir, tff.simFile)
          newFile = tff.simFile
          if 'qemu' in sg.executor:
            newFile = newFile.replace('.npy', '.bin')
          else:
            newFile = newFile.replace('.npy', '-simout.npy')
          if not os.path.isfile(newFile):
            if sgMode == 'all_available':
              print('WARNING: --check_against_ref all_available  ignoring missing file ', newFile)
            else:
              print('ERROR: --check_against_ref all_internal  missing file ', newFile)
              retAll = 1
          else:
            cmd = "%s/compiler/util/npy_diff_files %s --gold %s --new %s" % (
                    kPath, diffOptions, refFile, newFile)
            comparedFiles.append(newFile)
            print("INFO: executing %s" % cmd, flush=True)
            ret = os.system(cmd)
            retAll = ret | retAll
            print("INFO: subgraph  %s  internal tensor  %s  status  %s" % (
                    sg.sgDir, tensorName, ('PASS' if ret == 0 else 'FAIL')), flush=True)
      # if we are doing cachedKelf make sure we do that compare even if this fails
      if retAll != 0 and not cachedGoldensPath:
        return retAll

    if cachedGoldensPath:
      # In the check_against_ref=only_golden case, we don't do any comparison
      # against TF so we need to detect the golden files here.
      if sgMode == "only_golden":
        for f in os.listdir(cachedGoldensPath):
          if f.endswith("-out.npy"):
            print("INFO: Found cached golden ouput %s/%s, adding to compare list." % \
                (cachedGoldensPath, f))
            comparedFiles.append(f)

      print("\nINFO: nn_executor.check Comparing %d cached golden tensors" % (len(comparedFiles)), flush=True)
      for f in comparedFiles:
        refFile = "%s/%s" % (cachedGoldensPath, f)
        newFile = f
        cmd = "%s/compiler/util/npy_diff_files %s --gold %s --new %s" % (
                kPath, diffOptionsCached, refFile, newFile)
        print(getInfoDateStr(), " executing %s" % cmd, flush=True)
        ret = os.system(cmd)
        retAll = ret | retAll

      print("\nINFO: nn_executor. check trace notifications file")
      qemuCacheFile = "%s/trace_notifications.json" % (cachedGoldensPath)
      currentFile = "%s/trace_notifications.json" % (working_dir)
      cmd = "%s/tools/ni/ni_diff %s %s > log-nidiff.txt 2>&1" % (os.environ["KAENA_RT_PATH"], qemuCacheFile, currentFile)
      print("\nINFO: nn_executor. Issuing ni_diff cmd %s" % (cmd))
      ret = os.system(cmd) # For the time being not using the ret

      if retAll != 0:
        return retAll
    
    return 0

ret = 0
test_dir = os.getcwd()
os.chdir(working_dir)
nnExec = NnExec(nnGraphFile)
ret = nnExec.run()
if ret != 0:
  raise RuntimeError("ERROR: nn_executor has returned non-zero exit status, please check log-exec*.txt")
if (ret == 0) and (args.check_against_ref != "none"):
  ret = nnExec.check(args.check_against_ref, args.diff_options,
                     args.cached_goldens, args.diff_options_cached)
  if ret != 0:
    # Run the script to compare simout and HH golden files for Sealife.
    # Sealife has wavegraph.json in the test directory root.
    if os.path.exists(os.path.join(working_dir, '..', 'wavegraph.json')):
      os.chdir(test_dir)

      print('\nINFO: Comparing simout and HH golden numpy files to find the first failing tensor')

      # Generate golden numpy files
      cmd = os.path.join(aws_tonga_src_path, 'starfish', 'bin', 'hhsim') + ' --debug 1 --hh_json hh-layout_transform_pool-elim_strided_load.json --tensormap tensor_map.json --writeall > log-hh-sim-golden.txt'
      print('Executing ' + cmd, flush=True)
      ret = os.system(cmd)
      if ret != 0:
        print('ERROR: cannot generate HH golden numpy files')

      # Run the followings too when --verbose=2 can generate subtensors with distinct ref_file names.
      # $AWS_TONGA_SRC/starfish/bin/starfish --verbose 2 --hh_json hh-layout_transform_pool-elim_strided_load.json
      # $AWS_TONGA_SRC/kcc/compiler/be/compiler/compiler.exe --wavegraph wavegraph.json
      # $AWS_TONGA_SRC/kcc/runtime/util/nn_executor --kelf_dir . --tfpb trivnet_freeze.pb --check_against_ref all --input_files trivnet_input_1:0.npy --check_against_ref all_available --inkling_debugflags

      if ret == 0:
        cmd = 'python3 ' + os.path.join(aws_tonga_src_path, 'starfish', 'util', 'find_simout_mismatch.py')
        cmd += ' --wavegraph wavegraph.json'
        print('Executing ' + cmd, flush=True)
        os.system(cmd)
      
      print('See log-diff-layers.txt for details\n')

print(getInfoDateStr(), "Kaena RT status %s" % ("PASS" if ret == 0 else "FAIL"))
sys.exit(0 if ret == 0 else 1)



