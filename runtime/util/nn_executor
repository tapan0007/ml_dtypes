#!/usr/bin/env python3

# Copyright (C) 2018, Amazon.com. All Rights Reserved
#
# Kaena neural network executor with mapping to mixed backends.
#

import argparse
import os.path
import sys, json, re, getpass, random
import glob
import shutil
import socket, errno
kPath = os.environ.get('KAENA_PATH')
if kPath is None:
  kPath =''
sys.path.insert(0, kPath + "/compiler/")

sys.path.insert(0, "/usr/lib/python3.6/site-packages/")
import numpy as np
from tffe.NpTransforms import NpTrans as npt

print("\nINFO: started as  ", " ".join(sys.argv), flush=True)

parser = argparse.ArgumentParser()
parser.add_argument('--kelf_dir', help='kelf directory', default="")
parser.add_argument('--tfpb', help='TensorFlow freeze graph file', default="f.pb")
parser.add_argument('--input_files', help='Files to be passed in to the first subgraph', default=[], nargs='+')
parser.add_argument('--output_files', help='Specify output files for last subgraph', default=[], nargs='+')
parser.add_argument('--check_against_ref', help='Compare produced subgraph output npy files against compiler provided goldens in local directory; use none (defalt), last, all', default='none')
parser.add_argument('--working_dir', help='all output will be stored here', default="")
parser.add_argument('--inkling_debugflags', help='inkling extra args', default=[], nargs='*')
parser.add_argument('--env', help='Set envvars pairs var=val var1=val1. Used to pass experimental controls to low-level tools', nargs='+', default=[])

args = parser.parse_args()
kelf_dir = args.kelf_dir
kelf_dir = os.path.abspath(kelf_dir)
assert(kelf_dir != "")
nnGraphFile = kelf_dir + "/nn_graph.json"

# The --env support
if len(args.env) > 0:
  for varEqVal in args.env:
    var, val = varEqVal.split("=")
    os.environ[var] = val

input_files = [os.path.abspath(f) for f in args.input_files]
output_files = [os.path.abspath(f) for f in args.output_files]
working_dir = args.working_dir
if (working_dir == ""):
  working_dir = "%s/working_dir" % kelf_dir
working_dir = os.path.abspath(working_dir)
if not os.path.exists(working_dir):
  os.makedirs(working_dir)

print("INFO: using working directory  %s" % (working_dir))

def portInUse(port):
  inUse = True
  with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
    try:
      s.bind(("localhost", port))
      inUse = False
    except socket.error as e:
      if e.errno == errno.EADDRINUSE:
        print("INFO portInUse: Port %d is already in use" % port, flush=True)
      else:
        print("INFO portInUse: ", e, flush=True)
  return inUse

class NnSubgraph:
  def __init__(self, sgId, sgJson):
    self.sgId = sgId
    self.sgDir = sgJson["SubGraphDir"]
    self.inputs = sgJson["Inputs"]
    self.outputs = sgJson["Outputs"]
    self.executor = sgJson["executor"]
    self.execOptions = sgJson.get("execOptions", "")
    self.parallel_streams = sgJson.get("parallel_streams", False)
    self.cmd = ""
    self.cmd_args = ""
    if self.executor == "processor":
      self.cmd = sgJson["cmd"]
      self.cmd_args = sgJson["cmd_args"]

  # Translate and relink npy files that are inputs to this subgraph
  # Each subgraph produces output in the host format. So just rename npy
  # files A:0.npy to point to ../A:0-out.npy
  def getUpdatedInputs(self):
    newInputs = []
    if self.sgId == 0:
      return self.inputs

    for input in self.inputs:
      name = input["name"]
      f = input["file"]
      tfFile = "%s" % f.replace(".npy", "-out.npy")
      newInputs += [{"name" : name, "file" :tfFile}]

    return newInputs

  # Returns input file names in TPB format
  def createTpbFormatInputs(self):
    inputFilesInTpbFotmat = []
    for input in self.inputs:
      name = input["name"]
      f = input["file"]
      fBase = f[:-4]  # removes .npy
      # Only Fmaps come from the previous subgraph
      tpbInpFile = "%s_%s.npy"      % (fBase, npt.Formats[npt.SIM][npt.Fmaps])
      if self.sgId == 0:
        # The inputs for the first subgraph are same as when compiled
        tfFile = f
      else:
        tfFile = "%s-out.npy" % fBase

      if os.path.isfile(tpbInpFile):
        # Move the compiler generated input file
        origFile = tpbInpFile.replace(".npy", "-orig.npy")
        os.rename(tpbInpFile, origFile)

      # Translate and write the previous subgraph file
      if os.path.isfile(tfFile):
        inputShapeTf = input["shape"]
        # FIX_THIS: longterm we should store both TF and TBP IO formats in the nn_graph.json
        tfShapes = [None, npt.C, npt.NC, npt.HNC, npt.Formats[npt.TF][npt.Fmaps]]
        inputFormatTf = tfShapes[len(input['shape'])]
        if self.sgId == 0:
          # The inputs for the first subgraph are same as when compiled
          tpbInpFile = "%s/%s_%s_%s.npy" % (os.getcwd(), "trivnet", self.inputs[0]["name"], npt.Formats[npt.SIM][npt.Fmaps])

          ## # kaena-602: hack for testing one input file only; doesn't work for inferencing
          tpbInpPadSplitFile = tpbInpFile[:-4] + "_padsplit.npy"
          if os.path.isfile(tpbInpPadSplitFile):
            print("nn_executor: Replacing input npy ", tpbInpFile, " with padded input ", tpbInpPadSplitFile)
            tpbInpFile = tpbInpPadSplitFile
          else:
            print("nn_executor: Using original input npy ", tpbInpFile, ", did not find padded input ", tpbInpPadSplitFile)

          assert os.path.isfile(tpbInpFile)  # exists from compilation
        else:
          npt.formatNpyFileAs(tfFile, inputFormatTf, npt.Formats[npt.SIM][npt.Fmaps], outFile=tpbInpFile)
          print("INFO: translated TF format  %s  to TPB Fmap format  %s" % (tfFile, tpbInpFile))
        inputFilesInTpbFotmat.append(tpbInpFile)
      else:
        print("ERROR: no tf file not found %s" % tfFile, flush=True)
        sys.exit(-1)
    return inputFilesInTpbFotmat

  # There is no good way to identify consts (weights, biases, ..) in the kelf file yet.
  # This is a hack to link required files so that SIM can find them.
  def linkConsts(self):
    g = []
    g += glob.glob("%s/*%s*.npy"% (working_dir, npt.Formats[npt.SIM][npt.Weights]))
    g += glob.glob("%s/*%s*.npy"% (working_dir, npt.Formats[npt.SIM][npt.Fmaps]))
    for f in g:
      os.remove(f)
    g = []
    g += glob.glob("%s/sg*/*%s*.npy"% (kelf_dir, npt.Formats[npt.SIM][npt.Weights]))
    g += glob.glob("%s/sg*/*%s*.npy"% (kelf_dir, npt.Formats[npt.SIM][npt.Fmaps]))
    for f in g:
      dst_f = "%s/%s" % (working_dir, os.path.basename(f))
      if not os.path.isfile(dst_f) and not "out.npy" in f:
          os.symlink(f, dst_f )


  # Executes a subgraph, calls appropriate executor as specified in nn_graph.json
  # Returns 0 on success, -1 or error code of failure.
  def run(self):
    executor = self.executor
    logFile = "log-exec-%s-%s.txt" % (self.sgDir, executor)
    self.logfile = logFile

    if executor == "host":
      cmd = "PATH=%s/runtime/util:$PATH runtime_tf" % kPath
      cmd += "  --tfpb %s"  % "%s/tf.pb" %(kelf_dir)
      inputs = []
      for input in self.getUpdatedInputs():
        inputs += [input["name"]]
        inputs += [input["file"]]

      cmd += "  --input_tensors %s" % " ".join(inputs)
      outputs = []
      for output in self.outputs:
        outputs += [output["name"], output["file"].replace(".npy", "-out.npy")]
      cmd += "  --output_tensors %s" % " ".join(outputs)
    elif executor == "tcc" or executor == "wave":
      self.linkConsts()
      self.createTpbFormatInputs()
      cmd = "PATH=%s/runtime/util:$PATH runtime_sim" % kPath
      cmd += "  --tpb_dir %s" % "%s/%s" %(kelf_dir, self.sgDir)
      if self.parallel_streams:
        cmd += " --parallel_streams"
      cmd += ' --inkling_debugflags %s' % " ".join(args.inkling_debugflags)
    elif executor == "qemu_wave" or executor == "qemu_wave2":
      self.linkConsts()
      ifmapFiles = self.createTpbFormatInputs()
      user = getpass.getuser()
      hashStr = '%s-%s/%s' % (user, kelf_dir, self.sgDir)
      port = 5555 + abs(hash(hashStr)) % 8000
      while portInUse(port) and port < 60000:
        port += int(random.random()*100)
      print("INFO: qemu_wave hashed %s to qemu guest ssh port %d" % (hashStr, port))
      cmd  = "PATH=%s/runtime/util:$PATH qemu_rt" % kPath
      cmd += "  --action inference --kelf %s/%s" % (kelf_dir, self.sgDir)
      cmd += "  --sg %s" % self.sgDir
      cmd += "  --ifmaps %s" % "".join(map(str, ifmapFiles))
      qemuPool = os.getenv("KAENA_QEMU_RT_POOL", None)
      qemuZebu = os.getenv("KAENA_ZEBU_SERVER", None)
      if not qemuZebu == None:
        cmd += '  --zebu "%s"' % qemuZebu
      else:
        if qemuPool == None:
          cmd += "  --port %d" % port
        else:
          cmd += "  --pool %s" % qemuPool
      assert len(ifmapFiles) == 1 # qemu inkling supports single input  subgraphs only
      #cmd += ' --inkling_debugflags %s' % " ".join(args.inkling_debugflags)
    elif executor == "processor":
      inputs = []
      for input in self.getUpdatedInputs():
        inputs += [input["file"]]

      cmd = "%s/%s/%s" % (kelf_dir, self.sgDir, self.cmd)

      cmd += "  --inputs %s" % " ".join(inputs)
      if self.cmd_args:
        cmd += " " + self.cmd_args
      OutFile = ""
      if len(self.outputs) > 0:
        OutFile = self.outputs[0]["file"].replace(".npy", "-out.npy")
      if OutFile != "":
        cmd += "  --output %s" % (OutFile)
    elif executor == "waveopt":
      self.linkConsts()
      self.createTpbFormatInputs()
      cmd = "python3 %s/compiler/me/layeropt.py" % kPath
      cmd += "  --kgraph %s/%s/compiler.json" % (kelf_dir, self.sgDir)
      cmd += "  --inference"
    elif executor == "waveopt2":
      self.linkConsts()
      self.createTpbFormatInputs()
      cmd = "python3 %s/compiler/me/me_main.py" % kPath
      cmd += "  --kgraph %s/%s/compiler.json" % (kelf_dir, self.sgDir)
      cmd += "  --inference"
    cmd += "  " + self.execOptions
    cmd = "%s > %s 2>&1" % (cmd, logFile)
    print("\nINFO: executing %s" % cmd, flush=True)
    ret = os.system(cmd)
    if ret != 0:
      print("\nERROR: command  %s  returned non-zero exit code" % cmd, flush=True)
      return ret


    # Translate TPB format back to TF format.
    isWaveopt = (executor == "waveopt" or executor == "waveopt2")
    if (executor == "tcc" or  executor == "wave" or  executor == "qemu_wave" or executor == "qemu_wave2" or isWaveopt) and len(self.outputs) > 0:
      for jsonOutput in self.outputs:
        outNodeName, outFileRaw, outShape = [jsonOutput[k] for k in ['name', 'file', 'shape']]
        outFile = outFileRaw.replace(".npy", "-out.npy")
        outBase = outFileRaw[:-4]
        simExt = "simout"
        if isWaveopt:
          simExt = "midout"
        tpbFmapFormat = npt.Formats[npt.SIM][npt.Fmaps]
        tpbOutFile = "%s_%s-%s.npy" % (outBase, tpbFmapFormat, simExt)
        
        if executor == "qemu_wave" or executor == "qemu_wave2":
          # qemu_inkling runtime outputs raw binary portion of the ofmap in TPB format (not full npy)
          assert len(self.outputs) == 1   # qemu_inkling only support single output subgraphs
          outFileQuemu = "out-%s.bin" % self.sgDir
          print("INFO: nn_executor: translating Qemu output %s to %s" % (outFileQuemu, tpbOutFile))
          if os.path.isfile(outFileQuemu):
            outFileOrig = tpbOutFile.replace("-simout.npy", ".npy")
            ofmapOrig = np.load(outFileOrig)
            ofmapTpbShape = ofmapOrig.shape
            ofmapTpbDtype = ofmapOrig.dtype
            ofmapQuemu = np.fromfile(outFileQuemu, dtype=ofmapTpbDtype).reshape(ofmapTpbShape)
            np.save(tpbOutFile, ofmapQuemu)
          else:
            print("ERROR: nn_executor: missing Quemu Inkling output file %s" % outFileQuemu)
        
        if os.path.isfile(tpbOutFile):
          outShape = self.outputs[0]["shape"]
          npt.copyNpyFileAs(tpbOutFile, npt.SIM, npt.TF, npt.Fmaps, outFile=outFile, dstShape=outShape)
        else:
          print("ERROR: no tpb output file %s" % tpbOutFile, flush=True)
          return -1

    return 0
    

class NnExec:
  def __init__(self, nnGraphFile):
    with open(nnGraphFile) as fh:
      self.nnGraphJsonData = json.load(fh)
    self.subGraphs = []

  def update_io(self, input_files, output_files):
    sg_first = self.subGraphs[0]
    sg_last = self.subGraphs[-1]

    # Attach supplied input file to first subgraph
    if len(input_files) != 0:
      if len(sg_first.inputs) == 0:
        for i, input_file in enumerate(input_files):
          sg_first.inputs.append({"name" : "", "file" : input_file})
      else:
        assert (len(sg_first.inputs) == len(input_files))
        for i, input_file in enumerate(input_files):
          sg_first.inputs[i]["file"] = input_file

    # Attach supplied output file to last subgraph
    if len(output_files) != 0:
      if len(sg_last.outputs) == 0:
        sg_last.inputs.append({"name" : "", "file" : output_files[0]})
      else:
        assert (len(sg_last.outputs) == len(output_files))
        # we only support single output file for now
        sg_last.outputs[0]["file"] = output_files[0]

  def run(self):

    sgId = 0
    for sgJson in self.nnGraphJsonData["SubGraphs"]:
      sg = NnSubgraph(sgId, sgJson)
      self.subGraphs.append(sg)
      sgId += 1

    # copy input files to local working directory
    for f in input_files:
      shutil.copy(f, working_dir)

    self.update_io(input_files, output_files)

    for sg in self.subGraphs:
      ret = sg.run()
      if (ret != 0):
        return ret
    return 0


  # Return unix exit code. Subgraph mode can be "last" or "all"
  def check(self, sgMode):
    # find last non processor subgraph, compare output.
    sgList = []
    if sgMode == 'last':
      lastSg = None
      for sg in self.subGraphs:
        if sg.executor != "processor":
          lastSg = sg
      sgList = [lastSg]
    elif sgMode == 'all':
      sgList = self.subGraphs
    else:
      raise ValueError("ERROR: Unsupported subgraph mode --check_against_ref '%s'" % sgMode)
    for sg in sgList:
      for jsonOutput in sg.outputs:
        refOutFile = jsonOutput['file']
        outBase = refOutFile[:-4]
        flowOutFile = outBase + "-out.npy"
        cmd = "%s/compiler/util/npy_diff_files --gold %s/%s --new %s" % (kPath, kelf_dir, refOutFile, flowOutFile)
        print("INFO: executing %s" % cmd, flush=True)
        ret = os.system(cmd)
        if ret != 0:
          return ret
    return 0

ret = 0
os.chdir(working_dir)
nnExec = NnExec(nnGraphFile)
ret = nnExec.run()
if ret != 0:
  raise RuntimeError("ERROR: nn_executor has returned non-zero exit status, please check log-exec*.txt")
if (ret == 0) and (args.check_against_ref != "none"):
  ret = nnExec.check(args.check_against_ref)

print("INFO: Kaena RT status %s" % ("PASS" if ret == 0 else "FAIL"))
sys.exit(0 if ret == 0 else 1)



