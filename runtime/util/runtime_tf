#!/usr/bin/env python3

# Copyright (C) 2017, Amazon.com. All Rights Reserved
#
# Mini runtime environment for ofloading part of TF network to host
#
# Example:
#   Run 8-resnet50_fp16_keras_opt regression, cd there
#   How ro run the avg pool layer on host:
#     TO_DO: add input format conversion from TPB to TF format
#     $KAENA_PATH/runtime/util/runtime_tf --tfpb $KAENA_EXT_PATH/apps/tf/resnet50_keras/resnet50_fp16_keras_opt.pb --input_tensors activation_49/Relu:0 trivnet_activation_49__Relu:0.npy --output_tensor avg_pool/AvgPool:0 pool.npy
#     $KAENA_PATH/compiler/util/npy_diff_files --gold trivnet_avg_pool__AvgPool:0.npy --new pool.npy

import argparse
import os.path
import sys
import numpy as np
from tensorflow.python.platform import gfile
from tensorflow.core.framework import graph_pb2
from tensorflow.python.saved_model import tag_constants
import tensorflow as tf

print("\nINFO: started as  ", " ".join(sys.argv), flush=True)

parser = argparse.ArgumentParser()
parser.add_argument('--tfpb', help='TensorFlow freeze graph file', default="f.pb")
parser.add_argument('--input_tensors', help='Tensors to set, pairs tensorName tensorValue.npy ...',
                    default=[], nargs='+',)
parser.add_argument('--output_tensors', help='Output tensor to compute, pair tensorName tensorValue.npy',
                    default=[], nargs='+')

args = parser.parse_args()
inputTensors = args.input_tensors
outputTensors = args.output_tensors
assert(len(inputTensors) >= 2)
assert(len(outputTensors) >= 2)

tfpbFile = args.tfpb
if not os.path.exists(tfpbFile):
  raise("ERROR: missing --tfpb " + tfpbFile)

class RuntimeTf:
  
  def __init__(self):
    self.tfg = None
  
  def load(self, tfpbFile):
    self.tfpbFile = tfpbFile
    self.sess = None

    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    config.gpu_options.per_process_gpu_memory_fraction = 0.08

    if os.path.isdir(self.tfpbFile):
      # Load model checkpoint
      graph = tf.Graph()
      with graph.as_default():
        # Grow GPU memory as needed at the cost of fragmentation.
        self.sess = tf.Session(graph=graph, config=config)
        tf.saved_model.loader.load( self.sess, [tag_constants.SERVING], self.tfpbFile)
    else:
      self.tfg = graph_pb2.GraphDef()
      with gfile.FastGFile(tfpbFile,'rb') as f:
        self.tfg.ParseFromString(f.read())
      self.sess = tf.Session(config=config)
      with self.sess.as_default():
        tf.import_graph_def(self.tfg, name="")

  def run(self, inputTensors, outputTensors):
    self.inputTensors = inputTensors
    self.outputTensors = outputTensors
    with self.sess.as_default():
      graph = self.sess.graph
      
      # Inject input tensors
      feedDict = {}
      for tensorName,tensorFile in zip(inputTensors[::2], inputTensors[1::2]):
        inputTensor = graph.get_tensor_by_name(tensorName)
        inputValue = np.load(tensorFile)

        # Check whether the input is bfloat16, convert if needed
        if inputValue.dtype == '|V2':
          inputValue = np.frombuffer(inputValue.tobytes(), dtype=tf.bfloat16.as_numpy_dtype).reshape(inputValue.shape)

        feedDict[inputTensor] = inputValue
      
      # Collect results
      outputVars = []
      var2file = {}
      for tensorName,tensorFile in zip(outputTensors[::2], outputTensors[1::2]):
        outputVars.append(tensorName)
        var2file[tensorName] = tensorFile
      tfResults = self.sess.run(outputVars, feed_dict=feedDict)
      for (var, nd) in zip(outputVars, tfResults):
        if nd.size > 0:
          outputFile = var2file[var]
          np.save(outputFile, np.ascontiguousarray(nd))
          print("\nINFO: wrote tensor %s into %s" %
                (var, outputFile), flush=True)


runtime = RuntimeTf()
runtime.load(tfpbFile)
runtime.run(inputTensors, outputTensors)

