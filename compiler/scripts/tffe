#!/usr/bin/env python3

# Copyright (C) 2017, Amazon.com. All Rights Reserved
#
# Top level Tensorflow to Tonga compiler interface
#
# Examples
#
#   set tfpb =  $KAENA_PATH/../Kaena-external-opensource/apps/tf/resnet_v2_152/pb/resnet_v2_152_fp32.pb
#
#   python3 $KAENA_PATH/compiler/tffe/tffe.py --tfpb $tfpb --depth 5
#
#   python3 $KAENA_PATH/compiler/tffe/tffe.py --tfpb $tfpb --depth 5 --weights
#
#   python3 $KAENA_PATH/compiler/tffe/tffe.py --tfpb $tfpb --depth 5 --focus block3/unit_18
#
#   set img = $KAENA_PATH/../Kaena-external-opensource/apps/tf/resnet_v2_152/dog.jpg
#   $KAENA_PATH/compiler/tffe//tffe.py --tfpb $tfpb --depth 5 --images $img
#     firefox out_graph_ann.dot.svg
#     soffice  out_ops.csv  
#
#   See the Makefile for examples of running units tests


import argparse
import os.path
import sys, json, re
import shutil
import datetime

sys.path.insert(0, os.environ["KAENA_PATH"] + "/compiler/tffe")

import TfFrontEnd
import KgraphPartitions
import KaenaOpGraph
from package_mgr import KPackageManager
from MiscUtil import remove_write_permissions

def getInfoDateStr():
  return "INFO %s :" % str(datetime.datetime.now())

kPath = os.environ.get('KAENA_PATH')
print("\n", getInfoDateStr(), "started as  ", " ".join(["'"+word+"'" if " " in word else word for word in sys.argv]))

parser = argparse.ArgumentParser()
parser.add_argument('--tfpb', help='TensorFlow freeze graph file', default="f.pb")
parser.add_argument('--out_prefix', help='Prefix for output files', default="out_")
parser.add_argument('--weights', help='Generate weight files', dest="weights",
                    action="store_true",  default=False)
parser.add_argument('--images', help='Generate images (IFMAP and OFMAP files) for an input image', dest="images", default=None, nargs='+')
parser.add_argument('--input_constants', help='Additional constants appended to the framework feed dictionary during calibration, key, value, key, value ...', default={}, nargs='+')
parser.add_argument('--depth', help='Depth of layer name hierarchy to show in the dot output (add negative sign to combine highest two hierarchies)',
                    type=int, default=5)
parser.add_argument('--debug', help='Debug level, 1 minimal, 3 detailed op values ',
                    type=int, default=0)
parser.add_argument('--focus', help='Regular expression to filter a subset of nodes',
                    default=".*")
parser.add_argument('--focus_to', help='Filter out all nodes but the fanin of this node. The fanin includes the node itself.',
                    default=None)
parser.add_argument('--width', help='Highlight data paths wider than the width',
                    type=int, default=1000)
parser.add_argument('--verbose', help='Verbosity level, 0 default, 1 shows in/outputs, 2 TBD',
                    type=int, default=0)
parser.add_argument('--input_node', help='Input node in the neural network graph (where --images should be injected during calibration)',
                    nargs='+', default=[])
parser.add_argument('--sg_input_format', help='Subgraph input node format (original number of dimensions) to override default format, specified as alternating node and format: <node0> <format0> <node1> <format1>; check subgraphs for proper input node names',
                    nargs='+', default=[])
parser.add_argument('--dot_timeout', help='Timeout for planarization of op and flow graphs in Graphviz, default 60 sec ',
                    type=int, default=60)
parser.add_argument('--scheduler', help='Select scheduler method tcc or wave, wave2, qemu_wave, default is tcc',
                    default='tcc')
parser.add_argument('--schedule_options', help='Configuration passed to scheduler, e.g. opt1=v1,opt2=v2,..., default is None',
                    default='')
parser.add_argument('--batch', help='Batch override for late-binding networks',
                    type=int, default=1)
parser.add_argument('--partition', help='Partition into subgraphs; use from, from_multi, meauto, multi_tpb, or auto. The from_multi is followed by list of comma-separate cut nodes, e.g. from a,b c e,f,g . The default is none',
                    nargs='+', default=["suppauto"])
parser.add_argument('--adjust_node_color', help='Applied after --partition; Node color Node1 color1 ...',
                    nargs='+', default=[])
parser.add_argument('--executors', help='Specifies executors per subgraph, e.g., tcc 1 2 3 (implies rest on host, host 0 4 5), default ""',
                    nargs='+', default=[])
parser.add_argument('--parallel_streams', help='run execution engines in parallel', action='store_true', default=False)
parser.add_argument('--preprocessor', help='Specify preprocessor script to be part of inference run', default="")
parser.add_argument('--postprocessor', help='Specify postprocessor script to be part of inference run', default="")
parser.add_argument('--preprocessor-args', help='Options to pass to preprocessor', default="")
parser.add_argument('--postprocessor-args', help='Options to pas to postprocessor', default="")
parser.add_argument('--exclude_ops_from_capture', help='Regexp for op names that will not be captured. TF errors out when capturing cond-false branch', default=None)
parser.add_argument('--show_op_name_in_kgraph', help='Makes the dataflow kgraph graphical output more verbose', dest="show_op_name_in_kgraph",
                    action="store_true",  default=False)
parser.add_argument('--wavegraph_checks', help='Run the following checks during compile phase. The default is no checks', nargs='+', default=[])
parser.add_argument('--waive_wavegraph_checks', help='Exclude wavegraph checker status from determination of test status', action='store_true')
parser.add_argument('--level_order_seed', help='Seed for randomization of Kgraph node order in compiler.json for stress testing of downstream flow, default is none which means alphabetic sort', type=int, default=None)

args = parser.parse_args()

tfpbFile = args.tfpb
if not os.path.exists(tfpbFile):
  raise("ERROR: missing --tfpb " + tfpbFile)
if args.images != None and args.focus != ".*":
  print("WARNING: Using --images with --focus is expert mode only, make sure the focussed subgraph is capable of supporting the imput images")

def writeBackEndFiles(kGraph, outPrefix, verbose):
  fileList = []
  (refOutNpyFile, fileListJson) = kGraph.genCompilerJson("compiler.json", verbose)
  fileList += fileListJson
  fileList += [outPrefix + "graph_ann.dot.svg"]
  #kGraph.genCompilertgz(outPrefix + "compiler.tgz", list(set(fileList)))

inputNamesToFormat = {}
iformat = args.sg_input_format
if iformat != []:
  if (len(iformat)%2) != 0:
    print("ERROR: Length of --sg_input_format list needs to be even")
    exit(1)
  else:  
    inputNamesToFormat = dict(zip(iformat[0::2], iformat[1::2]))

debugLevel = args.debug
dotTimeout = args.dot_timeout
tffe = TfFrontEnd.TfFe(args.width, debugLevel, dotTimeout, args.batch, args.show_op_name_in_kgraph, inputNamesToFormat)
tffe.loadPb(tfpbFile, args.focus, args.focus_to)
kog = tffe.getKaenaOpGraph()
kog.setSchedulerMode(args.scheduler)
KaenaOpGraph.Config.Scheduler.waveoptOptions = args.schedule_options
KaenaOpGraph.Config.levelOrderSeed = args.level_order_seed
kog.writeDot(args.depth, args.out_prefix + "graph.dot", "svg")
ret = 0
if args.weights:
  tffe.writeWeights(args.out_prefix)
  
if args.images != None:
  tffe.writeImages(args.out_prefix, 
                   args.images, 
                   args.input_node, 
                   args.input_constants, 
                   args.exclude_ops_from_capture,
                   args.preprocessor, 
                   args.preprocessor_args)
  kog.identifyMainFlowEdges()
  kog.optimizeForInference()
  tffe.writeOpsCsv(args.out_prefix + "ops.csv")
  kog.writeDot(args.depth, args.out_prefix + "graph_ann.dot", "svg")

  kp = KgraphPartitions.KgraphPart(kog, debugLevel)
  sgJsonList = []
  kp.colorNodes(args.partition, args.adjust_node_color)
  kp.partitionByColor()
  kp.calcExecutorMap(args.scheduler, args.executors)

  sgId = 0
  jsonFileOptionGeneric = "--wavegraph wavegraph.json"
  jsonFileOption = {"tcc" : "--json compiler.json",
                    "wave" : jsonFileOptionGeneric,
                    "wave2" : jsonFileOptionGeneric
                    }
  parallelStreamsOption= ""
  if args.parallel_streams:
      parallelStreamsOption = "--parallel_streams"
  for sg in kp.getSubgraphs():
    sgDir = "sg%02d" % sgId;
    print("\n", getInfoDateStr(), "processing subgraph %s" % sgDir)
    if debugLevel > 1:
      sg.graph.print()
    os.makedirs(sgDir)
    os.chdir(sgDir)
    sg.addSideNodes(kog)
    sg.graph.levelize()
    sg.relinkNpFiles("..")
    sg.graph.setSchedulerMode(args.scheduler)
    if debugLevel > 1:
      sg.graph.print()
    sg.graph.writeDot(args.depth, args.out_prefix + "graph_ann.dot", "svg")
    executor = kp.getExecutorById(sgId)
    if not executor == 'host':
      writeBackEndFiles(sg.graph, args.out_prefix, args.verbose)
      meOk, fileList = sg.graph.runScheduler(args.out_prefix)
      if not executor == 'waveopt':
        if meOk:
          cmd = "%s/compiler/be/compiler/compiler.exe %s %s> log-be.txt 2>&1" % (kPath, jsonFileOption.get(args.scheduler, jsonFileOptionGeneric), parallelStreamsOption)
          print(getInfoDateStr(), "executing %s" % cmd, flush=True)
          kccRet = abs(os.system(cmd))
          ret += kccRet
          if kccRet == 0:
            # Wave graph checker
            if len(args.wavegraph_checks) > 0:
              waive_wave_checker = args.waive_wavegraph_checks
              checksStr = " ".join(["--%s-check" % c for c in args.wavegraph_checks])
              cmd = "%s/compiler/wave-checker/wc-build/bin/wave_graph_checker --stdout --wave-graph-file wavegraph.json %s > log-wc.txt 2>&1" % (kPath, checksStr)
              print(getInfoDateStr(), "executing %s" % cmd, flush=True)
              wcRet = abs(os.system(cmd))
              if (waive_wave_checker == False):
                ret += wcRet
              if wcRet != 0:
                if (waive_wave_checker == True):
                  print("WARNING: wave_graph_checker failed, see %s/log-wc.txt but note that this is waived for tracking purpose" % sgDir);
                else:
                  print("ERROR: wave_graph_checker failed, see %s/log-wc.txt" % sgDir);
          else:
            print("ERROR: compiler.exe failed");
        else:
          print("ERROR: skipping backend compilation since the mid scheduler failed", flush=True)
          ret += 1
    print('Generating kelf.tar.gz for the sub graph...')
    def_json = None
    if os.path.isfile("def.json"):
        def_json = json.loads(open("def.json", 'r').read())
        pkmgr = KPackageManager(def_json)
        pkmgr.gen_pkg()
    else:
        print('ERROR: No def.json file exist to generate tar package.')
    os.chdir("..")
    sgJson = sg.genExecutorGraphJson(sgDir)
    sgJson["executor"] = executor
    if executor == "waveopt":
      sgJson["execOptions"] = KaenaOpGraph.Config.Scheduler.waveoptOptions
      if args.scheduler == "wave2":
        # Adjust the nn_executor "waveopt" to "waveopt2" when using "wave2" (layeropt2.py) scheduler
        sgJson["executor"] += '2'
    sgJson["parallel_streams"] = False
    if args.scheduler == 'wave' or args.scheduler == 'wave2' or args.scheduler == 'qemu_wave':
        sgJson["parallel_streams"] = args.parallel_streams
    sgJsonList.append(sgJson)
    sgId += 1

  kp.reportOpsAndSizes()

  if ret == 0:
    KgraphPartitions.attachPrePost(sgJsonList, args.preprocessor, args.postprocessor,
                                   args.preprocessor_args, args.postprocessor_args)

    #package tfpb file in kelf.
    if os.path.isfile(tfpbFile):
      shutil.copy2(tfpbFile, "%s/%s"%(os.getcwd(), "tf.pb"))
    else:
      shutil.copytree(tfpbFile, "%s/%s"%(os.getcwd(), "tf.pb"))

    nnGraphFile = "nn_graph.json"
    with open(nnGraphFile, "w") as f:
      s = json.dumps({"SubGraphs" : sgJsonList}, indent=2, sort_keys=True)
      f.write(s)
    cmd = "%s/compiler/util/kelf2dot --json %s" % (kPath, nnGraphFile)
    print(getInfoDateStr(), "executing", cmd)
    os.system(cmd)
    import package_mgr
    package_mgr.create_kelf(model_dir=os.getcwd(), subgraphs=sgJsonList)

    # K-elf compilation is done. Make output directory read/execute only
    #if debugLevel > 0:
    #  remove_write_permissions(".")

with open("/proc/%d/status" % os.getpid()) as procFh:
  print(procFh.read(), flush=True)
print(getInfoDateStr(), "Kaena Compiler status %s" % ("PASS" if ret == 0 else "FAIL"))
sys.exit(0 if ret == 0 else 1)


